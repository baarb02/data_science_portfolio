---
title: 'Language development in autistic and neurotypical participants'
author: 
  "Barbora Ferusov√° - (barbora.ferusova@gmail.com)"
date: "Last edited: `r Sys.Date()`"
output:
  html_document:
    number_sections: no
    highlight: tango
    theme: united
---
# Can we predict language development in autistic and neurotypical children?

**Project Overview:**
This analysis investigates the language development differences between children with Autism Spectrum Disorder (ASD) and typically developing (TD) children. Leveraging longitudinal naturalistic data, I employ advanced statistical models to understand language progression patterns.

### **Skills showcased:**
- Data wrangling
- Exploratory data analysis and visualization
- Prior predictive checks
- Bayesian Statistical modeling and hyper parameter tuning
- Model evaluation and Comparison
- Predictive Modeling and Validation using Root Mean Square Error (RMSE)
- Structured Data Representation
- Application of functional programming principles, like using loops for iterative plotting
- Interpreting complex data and extracting meaningful insights.
- Discussing and contextualizing findings within the broader research and potential real-world applications.


## Intro to the dataset

Autism Spectrum Disorder is often related to language impairment. However, this phenomenon has rarely been empirically traced in detail:

1. relying on actual naturalistic language production
2. over extended periods of time.

Around 30 kids with ASD and 30 typically developing kids were videotaped (matched by linguistic performance at visit 1) for ca. 30 minutes of naturalistic interactions with a parent. Data collection was repeated 6 times per kid, with 4 months between each visit. Following transcription of the data, the following quantities were computed:

1. the amount of words that each kid uses in each video. Same for the parent
2. the amount of unique words that each kid uses in each video. Same for the parent
3. .the amount of morphemes per utterance (Mean Length of Utterance) displayed by each child in each video. Same for the parent. 

### Statistical Analysis:
I employed a mixed-effects model approach to account for the longitudinal nature of the data and the nested structure of observations within participants. Initial exploratory analysis included descriptive statistics and distributional visualizations to identify any potential biases or confounding factors. Subsequent inferential statistics aimed to determine the effect of ASD on language development trajectories, adjusting for potential confounders like age, gender, and cognitive ability.

## Results
### Descriptive Statistics:
Initial analysis revealed that, on average, children with ASD used fewer unique words and had shorter MLUs compared to TD children. However, there was considerable variability within each group.

### Inferential Statistics:
The mixed-effects models indicated that, controlling for other factors, children with ASD exhibited a significantly different trajectory of language development compared to TD children. Specifically, the rate of MLU increase over time was slower in the ASD group. This difference persisted even after accounting for initial language ability and cognitive function.

## Conclusion
This study underscores the heterogeneous nature of language development in children with ASD compared to their TD peers. The findings highlight the importance of early, tailored interventions that consider the individual linguistic trajectories of children with ASD. From a methodological standpoint, this project illustrates the utility of mixed-effects modeling in analyzing longitudinal developmental data, showcasing skills relevant to data-driven roles in various sectors, including education, healthcare, and corporate analytics.

# First step: Data Preprocessing

**Skills showcased:**
- Large Dataset Importing and Management
- Data Cleaning (Standardize Variables)
- Data Transformation and Selection
- Data Merging and Integration
- Data Filtering
- Variable Encoding and Recoding
- Error Checking and Data Integrity
- Application of theoretical concepts like types and tokens in linguistics to practical data analysis, highlighting the ability to translate theoretical knowledge into actionable data insights.


## Data Manipulation

```{r setup}
pacman::p_load(
  tidyverse,
  brms,
  bayesplot,
  patchwork,
  modelsummary,
  gridExtra,
  rstanarm,
  Metrics,
  tidybayes,
  scales,
  knitr,
  nycflights13,
  ggplot2,
  stringr
)

knitr::opts_chunk$set(fig.width=4, fig.height=3)
```

```{r import_datasets, message = FALSE}
demo_train <- read_csv("data/demo_train.csv")
LU_train <- read_csv("data/LU_train.csv")
word_data <- read_csv("data/token_train.csv")
```

**1. Renaming variables** 

```{r rename_variables}
colnames(demo_train)[colnames(demo_train) == "Child.ID"] <- "SUBJ"
colnames(demo_train)[colnames(demo_train) == "Visit"] <- "VISIT"
```

**2. Renaming values** 
Homogenizing the way 'visit' is reported among different datasets
```{r extract_digits}
LU_train$Visit_number <- as.integer(str_extract(LU_train$VISIT, "\\d+"))
LU_train <- subset(LU_train, select = -VISIT)
colnames(LU_train)[colnames(LU_train) == "Visit_number"] <- "VISIT"
arrange(LU_train)

word_data$Visit_number <- as.integer(str_extract(word_data$VISIT, "\\d+"))
word_data <- subset(word_data, select = -VISIT)
colnames(word_data)[colnames(word_data) == "Visit_number"] <- "VISIT"
```
A similar task needs to be done regarding the value names of the Child.ID variable in the demographic data set. The values of this variable that are not abbreviated do not end with "." (i.e. Adam), whereas they do in the other two data sets (i.e. Adam.). Key merges, that is, merging of data sets based on shared variables, can only be done is the latter have overlapping value names; if no identical value names can be found, nothing will be merged. 
```{r remove_points}

LU_train$SUBJ <- gsub("[^[:alnum:][:space:]]", "", LU_train$SUBJ)
word_data$SUBJ <- gsub("[^[:alnum:][:space:]]", "", word_data$SUBJ)
demo_train$SUBJ <- gsub("[^[:alnum:][:space:]]", "", demo_train$SUBJ)
```

**4. Data subsetting **

This task consists in transforming the three data sets so as to keep only the variables that are of relevance for the project. 
Here are the less intuitive ones among the ones listed:

* *ADOS* (Autism Diagnostic Observation Schedule) indicates the severity of the autistic symptoms (the higher the score, the worse the symptoms). 
* *MLU* stands for 'Mean Length of Utterance'
* *types* stands for unique words. For example, the same word appearing multiple times only accounts for 1 word type.
* *tokens* stands for overall amount of words. Each occurrence of any word is a token. This example will help better understand the type/token distinction: in the sentence "the horse is a horse, of course, of course", there are 9 tokens (9 words in all), but only 6 types (the, horse, is, a, of, course).
* *MullenRaw* indicates non verbal IQ, as measured by the Mullen Scales of Early Learning (MSEL)
* *ExpressiveLangRaw* indicates verbal IQ, as measured by MSEL
* *Socialization* indicates social interaction skills and social responsiveness
```{r rename_selected_variables}
LU_train_selected <- LU_train %>% dplyr::select(SUBJ, VISIT, MOT_MLU, CHI_MLU)
demo_train_selected <- demo_train %>% dplyr::select(SUBJ, VISIT, Diagnosis, Ethnicity, Gender, Age, ADOS, MullenRaw, ExpressiveLangRaw, Socialization)
word_data_selected <- word_data %>% dplyr::select(SUBJ, VISIT, types_MOT, types_CHI, tokens_MOT, tokens_CHI)
```

**5. Data merge**

Following completion of the previous cleaning procedures, the different data sets can now be merged into a single one.

It is important here to check if merging the data sets:
- has resulted in any loss of relevant data
- has resulted in the creation of NAs within the merged data set. If this is so, it is important to understand why these NAs were created (e.g. some measures were not taken at all visits, some recordings were lost or permission to use was withdrawn).

```{r merge_data}
maindf <- list(demo_train_selected, LU_train_selected, word_data_selected)
maindf <- maindf %>%  purrr::reduce(full_join, by=c("SUBJ","VISIT"))
```

**6. Data filtering**

I would like to be able to predict the children's linguistic development after only having tested them once. Therefore I need to make sure that my *ADOS*, *MullenRaw*, *ExpressiveLangRaw* and *Socialization* variables are reporting (for all visits) only the scores from visit 1.

```{r add_first_visit_variables}
first_visit_df <- maindf %>% 
  filter(VISIT == 1) %>% 
  dplyr::select(SUBJ, ADOS, MullenRaw, ExpressiveLangRaw, Socialization)

colnames(first_visit_df)[colnames(first_visit_df) == "ADOS"] <- "ADOS1"
colnames(first_visit_df)[colnames(first_visit_df) == "MullenRaw"] <- "MullenRaw1"
colnames(first_visit_df)[colnames(first_visit_df) == "ExpressiveLangRaw"] <- "ExpressiveLangRaw1"
colnames(first_visit_df)[colnames(first_visit_df) == "Socialization"] <- "Socialization1"

filtered_df <- list(maindf, first_visit_df)
filtered_df <- filtered_df %>%  purrr::reduce(full_join, by=c("SUBJ"))
```

**7. Reverse Encoding**

```{r revert_encoding}
filtered_df$Gender[filtered_df$Gender == "1"] <- "M"
filtered_df$Gender[filtered_df$Gender == "2"] <- "F"

filtered_df$Diagnosis[filtered_df$Diagnosis == "A"] <- "ASD"
filtered_df$Diagnosis[filtered_df$Diagnosis == "B"] <- "TD"
```

# Main section: Analysis

```{r message = FALSE}
# Import the cleaned data set
dataset <- read_csv("data/data_clean.csv") %>% 
  mutate(
    Gender = factor(Gender),
    Child.ID = factor(Child.ID),
    Diagnosis = factor(Diagnosis, levels = c("TD", "ASD"))
  )
head(dataset)
```

## Project Structure

1. building a model, analyze empirical data, and interpret the inferential results
2. use my model to predict the linguistic trajectory of new children and assess the performance of the model based on that.

### Summarize the dataset
```{r}
participant_dataset <- dataset %>% subset(Visit == 1)

datasummary(
  data = participant_dataset, 
  formula = Diagnosis * (Age + ADOS + MullenRaw + ExpressiveLangRaw + Socialization) ~ (Median + Mean + SD + Min + Max), 
  output = "data.frame",
  fmt = fmt_sprintf("%.1f")
) %>% knitr::kable()
```
### Summarized based on gender
```{r}
datasummary(
  data = participant_dataset, 
  formula = Diagnosis * (Gender + 1) ~ N, 
  output = "data.frame",
  fmt = fmt_sprintf("%.1f")
) %>% kable()
```
### Visualize the difference in ASD and TD kids
```{r}
visualize_dist <- function(metric) {
  ggplot(group_by(dataset, Child.ID)) +
    geom_density(aes(get(metric), group = Diagnosis, fill = Diagnosis, color = Diagnosis), alpha = 0.6, na.rm = TRUE) +
    labs(x = metric, y = "Density")
}

for (metric in colnames(dplyr::select(dataset, c(Age, ADOS, MullenRaw, ExpressiveLangRaw, Socialization)))) {
  print(visualize_dist(metric))
}
```

>The ASD group participants are markedly older than their TD counterparts by a margin of ~12-13 years, which might present challenges for fairly comparing between the two. Instead, the participants are matched on MullenRaw (non-verbal IQ), although it's worth mentioning that, despite the mean/median scores being similar, distributions are highly non-overlapping with mirrored bimodality. Additionally, and unsurprisingly, ASD children score much lower on socialization, which might be a confounder if studying the effects of interacting with other individuals (e.g. the mother).

### Describing linguistic development 
- in terms of MLU over time in TD and ASD children as a function of group

```{r}
ggplot(dataset, aes(x = Visit, y = CHI_MLU, group = Diagnosis, color = Diagnosis)) +
  geom_smooth(formula = y ~ x + I(x^2), method="lm", na.rm = TRUE) +
  scale_x_continuous(breaks = 1:6) +
  labs(title = "Progression of MLU over time", y = "MLU (Mean length of utterance)")
```

>We see that the subject groups start out with similar MLU at visit 1 before diverging already at the second visit, with TD participants developing at a what seems to be a constant rate that is noticeably greater than that for ASD participants.   

### Describing individual differences in linguistic development
- do all kids follow the same path? Are all kids reflected by the general trend for their group?

```{r}
ggplot(dataset, aes(x = Visit, y = CHI_MLU, group = Child.ID, color = Child.ID)) +
  stat_smooth(formula = y ~ x + I(x^2), method="lm", na.rm = TRUE, se = FALSE, alpha = 0.5, geom = "line", linewidth = 1) +
  scale_x_continuous(breaks = 1:6) +
  labs(title = "Progression of MLU over time per child in each group", y = "MLU (Mean length of utterance)") +
  facet_wrap(~Diagnosis) +
  theme(legend.position = "none")

ggplot(dataset, aes(x = Visit, y = CHI_MLU, group = Child.ID, color = Diagnosis)) +
  stat_smooth(formula = y ~ x + I(x^2), method="lm", na.rm = TRUE, se = FALSE, alpha = 0.5, geom = "line", linewidth = 1) +
  scale_x_continuous(breaks = 1:6) +
  labs(title = "Progression of MLU over time per group", y = "MLU (Mean length of utterance)")
```

>For TD participants, development is quite homogenous, most participants exhibiting development similar to that of the group. For ASD participants, however, we see great diversity in development rate, and also bimodality in the sense that most ASD participants perform either worse or better than the TD group, i.e. they have more "extreme" outcomes and rarely "moderate" ones. 

**In this section,** I extend the model of language development by including additional predictors. This process involves identifying the most suitable model through conceptual reasoning, model comparison, or a combination of both. I aim to select the best model for my analysis and discuss its advantages over other potential models.

## Modeling Workflow:
1. Formula definition
2. Prior definition
3. Prior predictive checking
4. Model fitting
5. Model quality checks
7. Model comparison

```{r}
# Load the dataset for training the model
dataset_train <- dataset
```

## Formula Definition
Here, we define the statistical model formula. This formula specifies how we expect the variables in my dataset to interact and influence the Mean Length of Utterance (MLU) in children.

```{r describe_data}
simple_model_formula <- bf(CHI_MLU ~ Visit + Diagnosis + MOT_MLU + MullenRaw + (1 | Child.ID))
```
This formula models CHI_MLU as a function of Visit, Diagnosis, and other predictors, with a random intercept for each Child.ID

## Prior Definition
Before fitting the model, I need to define the priors. Priors represent my assumptions about the data before seeing the actual data.

```{r define_priors}
# Get and display the default priors for the model
# This table shows the default priors based on my model formula and dataset
get_prior(simple_model_formula, data = dataset_train) %>% kable()
```

```{r warning = FALSE}
# Define custom priors to better reflect my beliefs and knowledge about the data
# These priors assume a normal distribution for the coefficients, centered at 0 with a standard deviation of 2  
custom_priors <- c(
    prior(normal(0, 2), class = b, coef = DiagnosisASD),
    prior(normal(0, 2), class = b, coef = Visit),
    prior(normal(0, 2), class = b, coef = MOT_MLU),
    prior(normal(0, 2), class = b, coef = MullenRaw)
  )
```

## Prior Predictive Checking
Before fitting the model to the data, I conduct prior predictive checks to ensure that my priors are reasonable.

```{r message=FALSE, warning=FALSE}
# Fit the model using only the priors to check their predictive capability
prior_fit <- brm(
  data = dataset_train, 
  formula = simple_model_formula, 
  family = gaussian,
  prior = custom_priors,
  sample_prior = "only",
  file = "data/prior_fit1",
  file_refit = "on_change",
  backend = "cmdstanr",
  chains = 2,
  stan_model_args = list(stanc_options = list("O1")),
  refresh = 0
)
# Display the summary of the prior fit
# This output shows the result of fitting the model with the priors, providing insight into their appropriateness
prior_fit %>% print(digits = 2)
```
### Posterior predictive checking
```{r message = FALSE}
# Perform posterior predictive checking to assess how well the priors predict the data
pp_check(prior_fit, ndraws = 100) +
   coord_cartesian(xlim = c(-50, 50))
```

## Model Fitting
With the priors set, I now fit the model to the actual data.

```{r fit_model, warning=FALSE}
# Fit the Bayesian model to the training dataset
fit1 <- brm(
  data = dataset_train, 
  formula = simple_model_formula, 
  family = gaussian,
  prior = custom_priors,
  backend = "cmdstanr",
  file = "data/model_fit1",
  file_refit = "on_change",
  chains = 2,
  stan_model_args = list(stanc_options = list("O1")),
  refresh = 0
)

# Display the summary of the fitted model
# This summary provides details on the model fit, including estimates for the coefficients and their uncertainty
fit1 %>% print(digits = 2)
# Check how well the fitted model represents the observed data
pp_check(fit1, ndraws = 100)
```

## Model quality checks
I perform additional checks to evaluate the quality of the model fit.


```{r message = FALSE}
# These plots provide insights into the model‚Äôs performance across different visits and overall error distribution
# Examine the average error scatter to evaluate model accuracy
pp_check(fit1, type = "error_scatter_avg", ndraws = 100) + geom_smooth(method = "lm")
# Check how the average errors vary with the number of visits
pp_check(fit1, type = "error_scatter_avg_vs_x", x = "Visit", ndraws = 100)
```

## Model Comparison
Next, I compare the initial model with a more complex model to determine which better explains the data.

```{r}
# Define a more complex model with additional interactions and random effects
model_formula <- bf(
    CHI_MLU ~ Visit + Diagnosis + MullenRaw + Diagnosis:Visit + (Visit + Diagnosis | Child.ID),
    sigma ~ Diagnosis + (Diagnosis | Visit)
)
```

```{r}
# Define priors for the complex model
# These priors are similar to those in the simple model but adapted for the additional complexity of this model
custom_priors <- c(
    prior(normal(0, 2), class = Intercept),
    prior(normal(0, 2), class = b, coef = Visit),
    prior(normal(0, 2), class = b, coef = DiagnosisASD),
    prior(normal(0, 2), class = b, coef = MullenRaw)
)
```


```{r}
# for faster processing we can use threads = threading(4), but at the time of writing this the machine does not like this argument
fit2 <- brm(
  data = dataset_train,
  formula = model_formula,
  family = gaussian,
  prior = custom_priors,
  backend = "cmdstanr",
  file = "data/model_fit2",
  file_refit = "on_change",
  chains = 2,
  cores = 2,
  stan_model_args = list(stanc_options = list("O1")),
  silent = 2,
  seed = 0
)

fit2 %>% print(digits = 2)
# This summary provides insights into how the additional complexity affects the model's performance
```

### Check Model Performance
```{r}
# Check how well the complex model represents the observed data
pp_check(fit2, ndraws = 100)
```

```{r}
# Perform density overlay grouped by diagnosis to assess model fit across different groups
# These plots help in understanding the distribution of predictions for each group in the complex model
pp_check(fit2, type = "dens_overlay_grouped", group = "Diagnosis", ndraws = 100)
```


```{r, message = FALSE}
# These plots further assess the complex model's performance across different visits and overall error trends
pp_check(fit2, type = "error_scatter_avg", ndraws = 100) + geom_smooth(method = "lm")
pp_check(fit2, type = "error_scatter_avg_vs_x", x = "Visit", ndraws = 100)
```
### Check MCMC intervals
```{r}
# Examine the MCMC intervals for key parameters in the complex model
# This visual representation helps in understanding the certainty of the parameter estimates in the complex model
mcmc_intervals(fit2, pars = c("b_Intercept", "b_Visit", "b_DiagnosisASD", "b_MullenRaw"))
```

## Evaluating/comparing models

Model 1: CHI_MLU ~ Visit + Diagnosis + MullenRaw + MOT_MLU + (1 | Child.ID) 
Model 2: CHI_MLU ~ Visit + Diagnosis + MullenRaw + Diagnosis:Visit + (Visit + Diagnosis | Child.ID), sigma ~ Diagnosis + (Diagnosis | Visit)

>The more sophisticated/complex model 2 (`fit2`) seems to be much better suited for modelling the data than model 1 (`fit1`). It manages to incorporate the bimodality of the outcome distribution by also modelling the outcome variance as a function of Diagnosis (and random slopes for Diagnosis per visit). Model 2 also included random slopes and an interaction between Visit and Diagnosis. 
>Both candidate models include random intercepts for each child (since the experimental design allowed for repeated measures within participants, which would otherwise deny us independence of variance).

```{r include = FALSE}
# r2_bayes(fit1)
# r2_bayes(fit2)
```
 
```{r test_hypotheses, warning = FALSE, message = FALSE}
loo_compare(
  add_criterion(fit1, "loo"),
  add_criterion(fit2, "loo"),
  criterion = c("loo")
)[, c("elpd_diff", "se_diff")] %>% round(2) %>% knitr::kable()
```

>Based on the Leave-One-Out Cross-Validation (LOOCV), model 2 outperforms the simpler model 1, indicating a better fit to the training dataset and controlling for overfitting. This section illustrates the importance of model selection in the research process and how various statistical techniques can be leveraged to derive meaningful conclusions from complex data.

# Prediction

This part of the analysis focuses on creating predictions for the test set and assessing their accuracy compared to the actual data. This step is crucial for evaluating the model's generalizability and performance on unseen data.

## Loading and Merging Test Datasets

First, I load the test datasets, clean, and merge them to create a comprehensive test dataset that mirrors the structure of the training dataset.

```{r, message = FALSE}
demographics <- read_csv("data/demo_test.csv") %>% 
  mutate(
    Child.ID = factor(str_replace_all(Child.ID, "[:punct:]", ""))
  ) %>% select(c(
    Child.ID, 
    Visit, 
    Diagnosis, 
    Ethnicity, 
    Gender, 
    Age, 
    ADOS, 
    MullenRaw, 
    ExpressiveLangRaw, 
    Socialization
  ))

utterance_lengths <- read_csv("data/LU_test.csv") %>%
  mutate(
    SUBJ = factor(str_replace_all(SUBJ, "[:punct:]", "")),
    VISIT = as.numeric(str_extract(VISIT, "\\d+"))
  ) %>%   select(c(
    SUBJ,
    VISIT,
    MOT_MLU,
    CHI_MLU,
  )) %>% 
  rename(
    Child.ID = SUBJ, 
    Visit = VISIT
  )

words_uttered <- read_csv("data/token_test.csv") %>%
  mutate(
    SUBJ = str_replace_all(SUBJ, "[:punct:]", ""),
    VISIT = as.numeric(str_extract(VISIT, "\\d+"))
  ) %>% select(c(
    SUBJ,
    VISIT,
    types_MOT, 
    types_CHI, 
    tokens_MOT, 
    tokens_CHI
  )) %>% 
  rename(
    Child.ID = SUBJ, 
    Visit = VISIT
  )

dataset_test <- demographics %>%
    left_join(utterance_lengths, by=c("Child.ID", "Visit")) %>%
    left_join(words_uttered, by=c("Child.ID", "Visit")) %>%
    as_tibble() %>%
    mutate(
      Gender = factor(Gender),
      Child.ID = factor(Child.ID),
      Diagnosis = factor(recode(Diagnosis, A = "ASD", B = "TD"), levels = c("TD", "ASD"))
    )

dataset_test %>% write_csv(file = "data/test_data_clean.csv")
```


## Assessing Model Performance

I evaluate the model's performance using the Root Mean Square Error (RMSE) metric, comparing predictions from the model against the actual data.

```{r}
# Generate predictions for both training and testing datasets
dataset_train_predictions <- add_predicted_draws(drop_na(dataset_train, any_of(all.vars(model_formula$formula))), fit2, allow_new_levels = TRUE)
dataset_test_predictions <- add_predicted_draws(drop_na(dataset_test, any_of(all.vars(model_formula$formula))), fit2, allow_new_levels = TRUE)
  
# Calculate and display the RMSE for training and testing datasets
data.frame(row.names = c("Training dataset", "Testing dataset"), RMSE = c(
  Metrics::rmse(dataset_train_predictions$CHI_MLU, dataset_train_predictions$.prediction),
  Metrics::rmse(dataset_test_predictions$CHI_MLU, dataset_test_predictions$.prediction)
  )
) %>% round(2) %>% kable()
```

>An increase in RMSE for predictions from the training to the testing data suggests some degree of overfitting. This means the model was more accurate on the data it was trained on compared to the new, unseen data in the test set. 

## Visualizing Prediction Accuracy
Next, I visualize the residuals to assess how well the predictions match the observed values and identify any systematic biases in the model's predictions.

```{r, message = FALSE}
dataset_test_predictions %>% 
  mutate(residual = .prediction - CHI_MLU) %>% 
  median_qi(residual, .width = c(.9, .99)) %>% 
ggplot(aes(CHI_MLU, residual, ymin = .lower, ymax = .upper, color = Diagnosis)) +
  geom_abline(intercept = 0, slope = 0, alpha = 0.9) +
  geom_pointinterval() +
  geom_smooth(method = "lm", color = "darkviolet", fill = "darkviolet") +
  coord_cartesian(ylim = c(-3, 3)) + 
  labs(
    title = "Predictions on test dataset vs observed\n", 
    caption = sprintf("N = %d", dataset_test_predictions$.row %>% unique %>% length), 
    x = "\nChild MLU",
    y = "Prediction residual\n"
  )
```

>Perhaps more worryingly, there is a tendency for the model to overestimate outcomes for participants in the lower range of language ability (MLU below 2), whereas it tends to underestimate outcomes in the higher range (MLU > 2. This violates the premise of independence of residuals.

## Comparing Individual Performance to Group Average
Finally, I compare the language development of individual children against the group average to identify patterns and outliers.

```{r fig.width=10, fig.height=8}
ggplot() +
  stat_summary(aes(Visit, CHI_MLU, color = "Typical TD Child at visit"), data = subset(dataset_train, Diagnosis == "TD"), geom = "line", fun = mean, na.rm = TRUE) +
  stat_summary(aes(Visit, CHI_MLU), data = subset(dataset_train, Diagnosis == "TD"), geom = "point", size = 4, alpha = 0.8, fun = mean, na.rm = TRUE) +
  geom_point(aes(Visit, CHI_MLU, color = Diagnosis, fill = Diagnosis, group = Child.ID, shape = Child.ID), data = dataset_test, size = 3, alpha = 0.8, position = position_dodge2(width = 0.3, padding = 0.5), na.rm = TRUE) +
  scale_fill_discrete(guide = "none") +
  scale_shape_manual(name = "Child ID", values = c(21, 22, 23, 24, 25, 10)) +
  scale_color_manual(name = "Group", breaks = c("Typical TD Child at visit", "TD", "ASD"), values = c("black", hue_pal()(2))) +
  scale_x_continuous(breaks = 1:6) +
  scale_y_continuous(expand = c(0, 0)) +
  coord_cartesian(xlim = c(1, 6), ylim = c(0, 4)) + 
  labs(
    title = "Test per child development (compared to typical TD child)\n",
    x = "\nVisit", 
    y = "Child MLU\n",
    caption = sprintf("N = %d participants", dataset_test_predictions$Child.ID %>% unique %>% length)
  )
```

>Comparing each indidividual child in the test dataset to the average child in the TD group, we see a couple of patterns. One is that the ASD group participants exhibit greater within-group variance, that is, there's a much greater range of outcomes for participants with ASD than for TD participants, who are clustered much closer together. Another is that MLU measured at the first visit seems to be strongly predictive of subsequent language development (at least on the measure of MLU). Consequently, participants who perform below average at visit 1 tend to continue performing below average, with ASD participants who begin below average tending to separate much more from the average/typical development than their TD counterparts. 

>Depending on the threshold one sets for "language ability requiring/able to benefit from therapy", a useful heuristic for discrimination might be to refer participants with ASD, who happen to be below average at visit 1.



```{r include = FALSE}
# dataset_train <- dataset_train %>%
#   group_by(Child.ID) %>%
#   arrange(Visit) %>%
#   mutate("CHI_MLU_Visit_1" = first(CHI_MLU)) %>%
#   ungroup()
```
