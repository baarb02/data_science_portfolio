---
title: 'Language development in autistic and neurotypical participants'
author: 
  - Barbora Ferusov√° - (barbora.ferusova@gmail.com) 
date: "Last edited: `r Sys.Date()`"
output:
  html_document:
    number_sections: no
    highlight: espresso
    theme: united
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width=4, fig.height=3)
knitr::opts_chunk$set(echo = TRUE, awarning = FALSE, message = FALSE)
knitr::opts_knit$set(root.dir = "/Users/barb/Documents/GitHub/data_science_portfolio/ASD_project/")


pacman::p_load(
  tidyverse,
  brms,
  bayesplot,
  patchwork,
  modelsummary,
  gridExtra,
  rstanarm,
  Metrics,
  tidybayes,
  scales,
  knitr
)
# you will have to install CmdStan with Anaconda if you dont have it 
theme_set(theme_minimal())
```

# Intro

Autism Spectrum Disorder is often related to language impairment. However, this phenomenon has rarely been empirically traced in detail:

1. relying on actual naturalistic language production
2. over extended periods of time.

Around 30 kids with ASD and 30 typically developing kids were videotaped (matched by linguistic performance at visit 1) for ca. 30 minutes of naturalistic interactions with a parent. Data collection was repeated 6 times per kid, with 4 months between each visit. Following transcription of the data, the following quantities were computed:

1. the amount of words that each kid uses in each video. Same for the parent
2. the amount of unique words that each kid uses in each video. Same for the parent
3. .the amount of morphemes per utterance (Mean Length of Utterance) displayed by each child in each video. Same for the parent. 

```{r message = FALSE}
dataset <- read_csv("data/data_clean.csv") %>% 
  mutate(
    Gender = factor(Gender),
    Child.ID = factor(Child.ID),
    Diagnosis = factor(Diagnosis, levels = c("TD", "ASD"))
  )
head(dataset)
```

## Project Structure

1. building a model, analyze empirical data, and interpret the inferential results
2. use my model to predict the linguistic trajectory of new children and assess the performance of the model based on that.

# Analysis

```{r}
participant_dataset <- dataset %>% subset(Visit == 1)

datasummary(
  data = participant_dataset, 
  formula = Diagnosis * (Age + ADOS + MullenRaw + ExpressiveLangRaw + Socialization) ~ (Median + Mean + SD + Min + Max), 
  output = "data.frame",
  fmt = fmt_sprintf("%.1f")
) %>% kable()
```

```{r}
datasummary(
  data = participant_dataset, 
  formula = Diagnosis * (Gender + 1) ~ N, 
  output = "data.frame",
  fmt = fmt_sprintf("%.1f")
) %>% kable()
```

```{r}
visualize_dist <- function(metric) {
  ggplot(group_by(dataset, Child.ID)) +
    geom_density(aes(get(metric), group = Diagnosis, fill = Diagnosis, color = Diagnosis), alpha = 0.6, na.rm = TRUE) +
    labs(x = metric, y = "Density")
}

for (metric in colnames(dplyr::select(dataset, c(Age, ADOS, MullenRaw, ExpressiveLangRaw, Socialization)))) {
  print(visualize_dist(metric))
}
```

>The ASD group participants are markedly older than their TD counterparts by a margin of ~12-13 years, which might present challenges for fairly comparing between the two. Instead, the participants are matched on MullenRaw (non-verbal IQ), although it's worth mentioning that, despite the mean/median scores being similar, distributions are highly non-overlapping with mirrored bimodality. Additionally, and unsurprisingly, ASD children score much lower on socialization, which might be a confounder if studying the effects of interacting with other individuals (e.g. the mother).

- Describing linguistic development (in terms of MLU over time) in TD and ASD children (as a function of group). 

```{r}
ggplot(dataset, aes(x = Visit, y = CHI_MLU, group = Diagnosis, color = Diagnosis)) +
  geom_smooth(formula = y ~ x + I(x^2), method="lm", na.rm = TRUE) +
  scale_x_continuous(breaks = 1:6) +
  labs(title = "Progression of MLU over time", y = "MLU (Mean length of utterance)")
```

>We see that the subject groups start out with similar MLU at visit 1 before diverging already at the second visit, with TD participants developing at a what seems to be a constant rate that is noticeably greater than that for ASD participants.   

- Describing individual differences in linguistic development: do all kids follow the same path? Are all kids reflected by the general trend for their group?

```{r}
ggplot(dataset, aes(x = Visit, y = CHI_MLU, group = Child.ID, color = Child.ID)) +
  stat_smooth(formula = y ~ x + I(x^2), method="lm", na.rm = TRUE, se = FALSE, alpha = 0.5, geom = "line", linewidth = 1) +
  scale_x_continuous(breaks = 1:6) +
  labs(title = "Progression of MLU over time per child in each group", y = "MLU (Mean length of utterance)") +
  facet_wrap(~Diagnosis) +
  theme(legend.position = "none")

ggplot(dataset, aes(x = Visit, y = CHI_MLU, group = Child.ID, color = Diagnosis)) +
  stat_smooth(formula = y ~ x + I(x^2), method="lm", na.rm = TRUE, se = FALSE, alpha = 0.5, geom = "line", linewidth = 1) +
  scale_x_continuous(breaks = 1:6) +
  labs(title = "Progression of MLU over time per group", y = "MLU (Mean length of utterance)")
```

>For TD participants, development is quite homogenous, most participants exhibiting development similar to that of the group. For ASD participants, however, we see great diversity in development rate, and also bimodality in the sense that most ASD participants perform either worse or better than the TD group, i.e. they have more "extreme" outcomes and rarely "moderate" ones. 

- Including additional predictors in my model of language development (N.B. not other indexes of child language: types and tokens, that'd be cheating). Identifying the best model, by conceptual reasoning, model comparison or a mix. Reporting the model I chose (and name its competitors, if any) and discussing why it's the best model.

Workflow:
1. Formula definition
2. Prior definition
3. Prior predictive checking
4. Model fitting
5. Model quality checks
7. Model comparison

```{r}
dataset_train <- dataset
```

## Formula Definition

```{r describe_data}
simple_model_formula <- bf(CHI_MLU ~ Visit + Diagnosis + MOT_MLU + MullenRaw + (1 | Child.ID))
```

## Prior Definition

```{r define_priors}
get_prior(simple_model_formula, data = dataset_train) %>% kable()
```

```{r warning = FALSE}
custom_priors <- c(
    prior(normal(0, 2), class = b, coef = DiagnosisASD),
    prior(normal(0, 2), class = b, coef = Visit),
    prior(normal(0, 2), class = b, coef = MOT_MLU),
    prior(normal(0, 2), class = b, coef = MullenRaw)
  )
```

## Prior Predictive Checking

```{r}
prior_fit <- brm(
  data = dataset_train, 
  formula = simple_model_formula, 
  family = gaussian,
  prior = custom_priors,
  sample_prior = "only",
  file = "data/prior_fit1",
  file_refit = "on_change",
  backend = "cmdstanr",
  chains = 2,
  stan_model_args = list(stanc_options = list("O1")),
  refresh = 0
)

prior_fit %>% print(digits = 2)
```

```{r message = FALSE}
pp_check(prior_fit, ndraws = 100) +
   coord_cartesian(xlim = c(-50, 50))
```

## Model Fitting

```{r fit_model}
fit1 <- brm(
  data = dataset_train, 
  formula = simple_model_formula, 
  family = gaussian,
  prior = custom_priors,
  backend = "cmdstanr",
  file = "data/model_fit1",
  file_refit = "on_change",
  chains = 2,
  stan_model_args = list(stanc_options = list("O1")),
  refresh = 0
)


fit1 %>% print(digits = 2)

pp_check(fit1, ndraws = 100)
```

## Model quality checks

```{r message = FALSE}
pp_check(fit1, type = "error_scatter_avg", ndraws = 100) + geom_smooth(method = "lm")
pp_check(fit1, type = "error_scatter_avg_vs_x", x = "Visit", ndraws = 100)
```

## Model Comparison

```{r}
model_formula <- bf(
    CHI_MLU ~ Visit + Diagnosis + MullenRaw + Diagnosis:Visit + (Visit + Diagnosis | Child.ID),
    sigma ~ Diagnosis + (Diagnosis | Visit)
)
```

```{r}
custom_priors <- c(
    prior(normal(0, 2), class = Intercept),
    prior(normal(0, 2), class = b, coef = Visit),
    prior(normal(0, 2), class = b, coef = DiagnosisASD),
    prior(normal(0, 2), class = b, coef = MullenRaw)
)
```


```{r compare_models}
# for faster processing we can use threads = threading(4), but at the time of writing this the machine does not like this argument
fit2 <- brm(
  data = dataset_train,
  formula = model_formula,
  family = gaussian,
  prior = custom_priors,
  backend = "cmdstanr",
  file = "data/model_fit2",
  file_refit = "on_change",
  chains = 2,
  cores = 2,
  stan_model_args = list(stanc_options = list("O1")),
  silent = 2,
  seed = 0
)

fit2 %>% print(digits = 2)

pp_check(fit2, ndraws = 100)
```

```{r}
pp_check(fit2, type = "dens_overlay_grouped", group = "Diagnosis", ndraws = 100)
```


```{r, message = FALSE}
pp_check(fit2, type = "error_scatter_avg", ndraws = 100) + geom_smooth(method = "lm")
pp_check(fit2, type = "error_scatter_avg_vs_x", x = "Visit", ndraws = 100)
```

```{r}
mcmc_intervals(fit2, pars = c("b_Intercept", "b_Visit", "b_DiagnosisASD", "b_MullenRaw"))
```

## Evaluating/comparing models

Model 1: CHI_MLU ~ Visit + Diagnosis + MullenRaw + MOT_MLU + (1 | Child.ID) 
Model 2: CHI_MLU ~ Visit + Diagnosis + MullenRaw + Diagnosis:Visit + (Visit + Diagnosis | Child.ID), sigma ~ Diagnosis + (Diagnosis | Visit)

>The more sophisticated/complex model 2 (`fit2`) seems to be much better suited for modelling the data than model 1 (`fit1`). It manages to incorporate the bimodality of the outcome distribution by also modelling the outcome variance as a function of Diagnosis (and random slopes for Diagnosis per visit). Model 2 also included random slopes and an interaction between Visit and Diagnosis. 
>Both candidate models include random intercepts for each child (since the experimental design allowed for repeated measures within participants, which would otherwise deny us independence of variance).

```{r include = FALSE}
# r2_bayes(fit1)
# r2_bayes(fit2)
```
 
```{r test_hypotheses, warning = FALSE, message = FALSE}
loo_compare(
  add_criterion(fit1, "loo"),
  add_criterion(fit2, "loo"),
  criterion = c("loo")
)[, c("elpd_diff", "se_diff")] %>% round(2) %>% kable()
```

>Judging by LOOCV (Leave-One-Out Cross-Validation), we find that model 2 does indeed seem far superior to the simpler model 1 in describing/capturing the patterns in the traning dataset, even when attempting to control for overfitting. 

# Prediction

Creating predictions for the test set and assess how well they do compared to the actual data.

First, I have to load test dataframes and merge them

```{r import_data, message = FALSE}
demographics <- read_csv("data/demo_test.csv") %>% 
  mutate(
    Child.ID = factor(str_replace_all(Child.ID, "[:punct:]", ""))
  ) %>% select(c(
    Child.ID, 
    Visit, 
    Diagnosis, 
    Ethnicity, 
    Gender, 
    Age, 
    ADOS, 
    MullenRaw, 
    ExpressiveLangRaw, 
    Socialization
  ))

utterance_lengths <- read_csv("data/LU_test.csv") %>%
  mutate(
    SUBJ = factor(str_replace_all(SUBJ, "[:punct:]", "")),
    VISIT = as.numeric(str_extract(VISIT, "\\d+"))
  ) %>%   select(c(
    SUBJ,
    VISIT,
    MOT_MLU,
    CHI_MLU,
  )) %>% 
  rename(
    Child.ID = SUBJ, 
    Visit = VISIT
  )

words_uttered <- read_csv("data/token_test.csv") %>%
  mutate(
    SUBJ = str_replace_all(SUBJ, "[:punct:]", ""),
    VISIT = as.numeric(str_extract(VISIT, "\\d+"))
  ) %>% select(c(
    SUBJ,
    VISIT,
    types_MOT, 
    types_CHI, 
    tokens_MOT, 
    tokens_CHI
  )) %>% 
  rename(
    Child.ID = SUBJ, 
    Visit = VISIT
  )

dataset_test <- demographics %>%
    left_join(utterance_lengths, by=c("Child.ID", "Visit")) %>%
    left_join(words_uttered, by=c("Child.ID", "Visit")) %>%
    as_tibble() %>%
    mutate(
      Gender = factor(Gender),
      Child.ID = factor(Child.ID),
      Diagnosis = factor(recode(Diagnosis, A = "ASD", B = "TD"), levels = c("TD", "ASD"))
    )

dataset_test %>% write_csv(file = "data/test_data_clean.csv")
```

Assessing the performance of the model on the training vs. testing data: root mean square error is a good measure.

```{r assess_performance}
dataset_train_predictions <- add_predicted_draws(drop_na(dataset_train, any_of(all.vars(model_formula$formula))), fit2, allow_new_levels = TRUE)
dataset_test_predictions <- add_predicted_draws(drop_na(dataset_test, any_of(all.vars(model_formula$formula))), fit2, allow_new_levels = TRUE)
  
data.frame(row.names = c("Training dataset", "Testing dataset"), RMSE = c(
  Metrics::rmse(dataset_train_predictions$CHI_MLU, dataset_train_predictions$.prediction),
  Metrics::rmse(dataset_test_predictions$CHI_MLU, dataset_test_predictions$.prediction)
  )
) %>% round(2) %>% kable()
```

>An increase in RMSE for predictions going from the training to the testing data indicates some amount of overfitting which is to be expected. That is, the model was more accurate for the data on which it was trained, and by contrast, saw larger residuals for the test dataset, which includes data "unseen" during the training phase. 

```{r, message = FALSE}
dataset_test_predictions %>% 
  mutate(residual = .prediction - CHI_MLU) %>% 
  median_qi(residual, .width = c(.9, .99)) %>% 
ggplot(aes(CHI_MLU, residual, ymin = .lower, ymax = .upper, color = Diagnosis)) +
  geom_abline(intercept = 0, slope = 0, alpha = 0.9) +
  geom_pointinterval() +
  geom_smooth(method = "lm", color = "darkviolet", fill = "darkviolet") +
  coord_cartesian(ylim = c(-3, 3)) + 
  labs(
    title = "Predictions on test dataset vs observed\n", 
    caption = sprintf("N = %d", dataset_test_predictions$.row %>% unique %>% length), 
    x = "\nChild MLU",
    y = "Prediction residual\n"
  )
```

>Perhaps more worryingly, there is a tendency for the model to overestimate outcomes for participants in the lower range of language ability (MLU below 2), whereas it tends to underestimate outcomes in the higher range (MLU > 2. This violates the premise of independence of residuals.

```{r child_performance_td_average}
ggplot() +
  stat_summary(aes(Visit, CHI_MLU, color = "Typical TD Child at visit"), data = subset(dataset_train, Diagnosis == "TD"), geom = "line", fun = mean, na.rm = TRUE) +
  stat_summary(aes(Visit, CHI_MLU), data = subset(dataset_train, Diagnosis == "TD"), geom = "point", size = 4, alpha = 0.8, fun = mean, na.rm = TRUE) +
  geom_point(aes(Visit, CHI_MLU, color = Diagnosis, fill = Diagnosis, group = Child.ID, shape = Child.ID), data = dataset_test, size = 3, alpha = 0.8, position = position_dodge2(width = 0.3, padding = 0.5), na.rm = TRUE) +
  scale_fill_discrete(guide = "none") +
  scale_shape_manual(name = "Child ID", values = c(21, 22, 23, 24, 25, 10)) +
  scale_color_manual(name = "Group", breaks = c("Typical TD Child at visit", "TD", "ASD"), values = c("black", hue_pal()(2))) +
  scale_x_continuous(breaks = 1:6) +
  scale_y_continuous(expand = c(0, 0)) +
  coord_cartesian(xlim = c(1, 6), ylim = c(0, 4)) + 
  labs(
    title = "Test per child development (compared to typical TD child)\n",
    x = "\nVisit", 
    y = "Child MLU\n",
    caption = sprintf("N = %d participants", dataset_test_predictions$Child.ID %>% unique %>% length)
  )
```

>Comparing each indidividual child in the test dataset to the average child in the TD group, we see a couple of patterns. One is that the ASD group participants exhibit greater within-group variance, that is, there's a much greater range of outcomes for participants with ASD than for TD participants, who are clustered much closer together. Another is that MLU measured at the first visit seems to be strongly predictive of subsequent language development (at least on the measure of MLU). Consequently, participants who perform below average at visit 1 tend to continue performing below average, with ASD participants who begin below average tending to separate much more from the average/typical development than their TD counterparts. 
>Depending on the threshold one sets for "language ability requiring/able to benefit from therapy", a useful heuristic for discrimination might be to refer participants with ASD, who happen to be below average at visit 1.

```{r include = FALSE}
# dataset_train <- dataset_train %>%
#   group_by(Child.ID) %>%
#   arrange(Visit) %>%
#   mutate("CHI_MLU_Visit_1" = first(CHI_MLU)) %>%
#   ungroup()
```
