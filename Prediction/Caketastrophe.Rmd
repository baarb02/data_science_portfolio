---
title: 'Portfolio Assignment 4 (Individual)'
author: 
  - Barbora Ferusová - (202207346@post.au.dk) 
date: "`r Sys.Date()`"
output:
  pdf_document: 
    fig_height: 4
    fig_caption: yes
  word_document: default
header-includes: 
  \usepackage{fancyhdr}
  \usepackage{fvextra}
---


\pagestyle{fancy}
\fancyhead[HL]{Portfolio Assignment 4}
\fancyhead[HC]{Caketastrophe}
\fancyhead[HR]{Barbora Ferusová}

\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}


```{r, libraries, include=FALSE}
library(lme4)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(pastecs)
library(gridExtra)
library(DHARMa)
library(car)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = FALSE)
knitr::opts_knit$set(root.dir = '/work/CogSci_Methods01/portfolio_assignment_04-baarb02/.github')
```

```{r clear_workspace, include=FALSE}
rm(list=ls())
```

```{r warning=FALSE}
setwd('/work/CogSci_Methods01/data/')

cake_df <- read_csv('cake.csv',col_types = c(replicate = "f",recipe = "f", angle = "i",temp = "f")) %>% 
          select(-c(temperature))

titanic_df <- read_csv('titanic.csv',col_types = c(Survived = "f",Pclass = "f",Name = "f",Sex = "f",Age = "n")) %>% 
          select(-c("Siblings/Spouses Aboard", "Parents/Children Aboard", "Fare")) 

```

# 1. Cake Breakage Analysis

We used R (4.2.0) and lmerTest (3.1-3) to perform a linear mixed effects analysis of the relationship between breakage angle and temperature, considering the apparent effects of baking temperature on cake textures. As fixed effects, we entered replicate, or the number of repetitions. As random effects, we had intercepts for recipes. 

The model had the following syntax:

-   **Eq 1: angle \~ temp + replicate + (1 \| recipe )**

Outcome: angle
Fixed effects: temperature and replicate
Random effects: recipe (random intercept)

The assumption is that different combinations of ingredients were not something we were interested in analyzing. The choice of recipes was limited in this experiment and we cannot account for each ingredient and it's ratio. For what we know the specifics could be interfering with our analysis and therefore have to be selected as error. Other models were tested but either a. were an overfit, b. had a high AIC score or c. weren't theoretically justifiable.

The maximal model accounted for roughly 68% of variance in the angle (R^2^ conditional = 0.68),while the fixed effect accounted for roughly 11% (R^2^ marginal = 0.11). Visual inspection of residual plots did not reveal any obvious deviations from homoscedasticity or linearity. Angle has been found to significantly be modulated by number of replications (SE= 1.54, p \<.05) and temperature (SE= 0.97 , p \<.05).

```{r assessing what variables to use, echo=FALSE, include=TRUE}
plot(cake_df)
# we can consider all predictors
cake_00 <- lm(angle ~ temp + recipe + replicate, cake_df)# recipe does not seem to add significance
```

```{r trying out different models}
cake_0 <- lm(angle ~ temp, cake_df) 
#cake_1<-lmerTest::lmer(angle ~ temp + (1|recipe), data = cake_df, REML = F) # singular warning = its an overfit # bad R2

#cake_2<-lmerTest::lmer(angle ~ temp + (1+recipe|replicate), data = cake_df, REML = F) # singular warning = its an overfit # but highest R2 

#cake_3<-lmerTest::lmer(angle ~ temp + (1|replicate), data = cake_df, REML = F)

#cake_4<-lmerTest::lmer(angle ~ temp + (1|replicate) + (1|recipe), data = cake_df, REML = F)

#cake_5<-lmerTest::lmer(angle ~ temp + replicate + (1+ recipe|replicate), data = cake_df, REML = F) # low AIC 1640 # singular (overfit)

#cake_6<-lmerTest::lmer(angle ~ temp + recipe + (1|replicate), data = cake_df, REML = F) #padlet Anna

   cake_7<-lmerTest::lmer(angle ~ temp + replicate + (1|recipe),
                                      data = cake_df, REML = F)  # AIC of 1639 and no warning

   # cake_8<-lmerTest::lmer(angle ~ temp + replicate + (1+ temp|recipe),  data = cake_df, REML = F) # singular (overfit)
   
summary (cake_7)
summary (cake_0)

AIC(cake_0)
MuMIn::r.squaredGLMM(cake_0)
```

```{r echo=FALSE, warning=FALSE, include=TRUE}
AIC <- AIC(cake_7)
r2 <- MuMIn::r.squaredGLMM(cake_7) # 68% 
summary <- cbind(AIC, r2)
knitr::kable(summary, caption = "Model 7: angle ~ temp + replicate + (1|recipe)")
# this model says that recipe we chose explained some of the variance as a random intercept effect
# I chose a simpler model to avoid  overfitting the data
```

```{r include=TRUE, echo=FALSE}
#table of results
model7 = lmerTest::lmer(angle ~ temp + replicate + (1|recipe), data = cake_df, REML = F)

knitr::kable(summary(model7)$coefficients, caption = "Model 7: angle ~ temp + replicate + (1|recipe)", digits = c(4,4,4,4,30))

```

```{r include=TRUE, echo=FALSE}
#checking for linearity and homoschedasticity
plot(model7)
```

```{r testing the residuals with DHARMa, echo=FALSE, include=TRUE}
#DHARMa - checking residuals
simulationOutput <- simulateResiduals(fittedModel = model7, plot = F)
#residuals(simulationOutput, quantileFunction = qnorm, outlierValues = c(-7,7))
plot(simulationOutput) # red stars show simulation outliers (data points that are outside the range of simulated values)
#testOutliers(simulationOutput) # not significant
#testUniformity(simulationOutput) # not significant
```

# 2.1 Titanic Survival Analysis

We used R (4.2.0) and stats (version 3.6.2) to perform a logistic regression analysis of the relationship between survival and the present demographic information (age, sex and class). The outcome showed that our model had the accuracy of 79.48% in predicting the probability of survival based on the other demographic variables used.

The model had the following syntax:

-   Eq 2: Survived \~ Age + Sex + Pclass

Visual inspection of residual plots did not reveal any obvious deviations from normality, linearity or  multicollinearity. The sensitivity is 70.76%. The specificity is 84.95%. The model has less excess variance than the null model.

In conclusion, these values of accuracy are better than what would be obtained by chance. It therefore points us to the consensus that information such as age, sex and class can be used to successfully predict the survival of a catastrophe similar to the Titanic.

```{r anonamysing}
titanic_df$Name <- as.factor(titanic_df$Name)
titanic_df$Name <- as.numeric(titanic_df$Name)
titanic_df$Name <- as.factor(titanic_df$Name)

```

```{r echo=FALSE, include=TRUE}
ggplot(titanic_df, aes(Survived, Sex, color = Pclass, shape = Pclass)) + geom_jitter(width = .4, height = .4) + ggtitle("Using geom_jitter")
```


```{r}
plot(titanic_df)
```

```{r}
titanic_df <- titanic_df %>% 
                  mutate(isMedianAge = titanic_df$Age == median(titanic_df$Age) )
titanic_df$isMedianAge <- as.factor(titanic_df$isMedianAge)
```


```{r picking a model, echo=FALSE}
general_model <- glm(Survived ~ Sex + Age + Pclass, family = binomial(link=logit),
                                      data = titanic_df)


tit_1 <-glm(Survived ~ Sex + isMedianAge + Pclass, family = binomial(link=logit),
                                      data = titanic_df)
summary(tit_1)

knitr::kable(summary(general_model)$coefficients, caption = "Logistic regression model: Survived ~ Sex + Age + Pclass")
#vif(tit_1)
#logLik(tit_1)
# AIC of 793
# less unexplained variance than the null model 
```



```{r include= TRUE, echo=FALSE, message=FALSE, warning=FALSE}
model1 = glm(Survived ~ Age + Sex + Pclass , family = binomial(link=logit),
                                      data = titanic_df)
plot(model1, 1) # close to linear
```

### Testing for problematic multicollinearity

```{r echo=FALSE, include= TRUE}
vif<-car::vif(model1)
knitr::kable(vif)
```
The model does not show any concerning multicollinearity.


```{r include=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
plot(model1, 2) # close to normal 
```

```{r}
levels(titanic_df$Survived) 
levels(titanic_df$Pclass) 
```




## 2.2 Probabilities of different categories

```{r echo=TRUE, include=TRUE}
dummy.coef(tit_1) # getting coefficents of a version of the general model where we plug in the median age 
# (Survived ~ Sex + isMedianAge + Pclass)
```
Converting the log-odds estimates to probabilities and into percentages:
```{r echo=TRUE, include=TRUE}

Female_actual_prob <-boot::inv.logit(2.6360 + (-2.2224))*100

Male_actual_prob <-boot::inv.logit(0 + (-2.2224))*100 

Class3_actual_prob <-boot::inv.logit(0 + (-2.2224))*100 

Class1_actual_prob <-boot::inv.logit(1.8925 + (-2.2224))*100  

Class2_actual_prob <-boot::inv.logit(1.0633 + (-2.2224))*100 

Age_actual_prob <-boot::inv.logit(-0.4430 + (-2.2224))*100 
```


```{r include = TRUE, echo = FALSE}

alive_or_not_actual <-rbind(Female_actual_prob, Male_actual_prob, Class1_actual_prob, Class2_actual_prob, Class3_actual_prob, Age_actual_prob)

knitr::kable(alive_or_not_actual, caption = "Real probabilities of survival for different categories")
```



## 2.3 Bonus: Predicting probabilities with our model


```{r predicting probability of survival for every data point}
#Survived ~ Age + Sex + Pclass
#predicting probabilities
predicted_probs <- predict(general_model, titanic_df, type = 'response', na.action = na.pass)

#extracting actual data
actual <- titanic_df$Survived

#predicted probabilities of surviving against the actual data
pred_df <- tibble(predicted_probs, actual)

# readability
pred_df <- pred_df %>% 
  mutate(predicted_category = if_else(predicted_probs < 0.5, "0", "1"))
pred_df <- pred_df %>% 
  mutate(predicted_category = as_factor(predicted_category))

head(pred_df)

```

```{r confusionMatrix, include = TRUE, echo=FALSE, message=FALSE, warning=FALSE}
confusionMatrix <- caret::confusionMatrix(pred_df$predicted_category, pred_df$actual, positive = "1")
knitr::kable(confusionMatrix$overall, caption = "Confusion Matrix and Statistics")
```

```{r}
knitr::kable(confusionMatrix$byClass, caption = "Confusion Matrix and Statistics")
```


```{r different predictions, echo=TRUE, include=TRUE}
# predicting 
titanic_df_model <- titanic_df %>% 
  mutate (predicted_probs = predict(general_model, titanic_df, type = 'response', na.action = na.pass))

Female <- titanic_df_model  %>% 
  filter (Sex == "female")%>% 
  summarise(category = "Female", probability = mean(predicted_probs) *100)

Male <- titanic_df_model  %>% 
  filter (Sex == "male") %>% 
  summarise(probability = mean(predicted_probs) *100, category = "Male")

Class1 <- titanic_df_model  %>% 
  filter (Pclass== 1)%>% 
  summarise( category = "Class1", probability = mean(predicted_probs) *100)

Class2<- titanic_df_model  %>% 
  filter (Pclass == 2) %>% 
  summarise(category = "Class2", probability = mean(predicted_probs) *100)

Class3<- titanic_df_model  %>% 
  filter (Pclass == 3) %>% 
  summarise(category = "Class3", probability = mean(predicted_probs) *100)

Median_Age <- titanic_df_model %>% 
  filter (Age == 28) %>% 
   summarise(category = "Median Age", probability = mean(predicted_probs) *100)
  
```

```{r include = TRUE, echo = FALSE}
alive_or_not <-rbind(Female, Male, Class1, Class2, Class3, Median_Age)
knitr::kable(alive_or_not, caption = "Predicted probabilities of survival for different categories")
```



